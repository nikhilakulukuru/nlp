# nlp

##Supervised and Unsupervised Learning Methods on 10K and 10Q Data

### ABSTRACT

In this project, we are addressing a financial problem related to the stock price movement of the S&P 500 companies using their 2022 year-end 10K filings. This problem is intriguing and important because understanding the relationship between financial disclosures and stock price movements can offer valuable insights for investors and stakeholders. NLP techniques are highly relevant for this task, as they can extract and quantify subtle semantic cues and patterns from the vast textual data in 10-K filings, potentially highlighting predictive indicators of stock performance.

To tackle this project, we are drawing upon 10K filings of companies listed in the S&P 500 from the U.S. Securities and Exchange Commission’s (SEC) Electronic Data Gathering, Analysis, and Re- trieval (EDGAR) system. These documents are pivotal as they contain a wealth of information, including financial summaries, managerial insights, and potential risks for each fiscal year. Given our goal to forecast stock price movements for 2022 based on these filings, their content is directly pertinent. The data can be accessed through the SEC’s EDGAR platform: https://www.sec. gov/edgar.shtml. In this problem, we specifically look at only the MDMA (Management Dis- cussion and Analysis) section of the 10-K filings. We particularly look into the MDMA section since it provides an overview of the company’s financial performance during the reporting period. This section also discusses any changes or trends in the company’s financials. This can help us under- stand how the company will perform in the near future.

We are leveraging unsupervised learning methods to gain deeper insights into our data. Our initial approach involves generating word clouds to identify the most frequent terms within documents associated with target variables ’1’ and ’0.’ Furthermore, we employ clustering algorithms to seg- ment our data and Latent Semantic Analysis (LSA) to uncover key topics within each cluster. These unsupervised techniques enable us to assess whether distinct patterns emerge among companies with rising or falling stock prices, which can serve as valuable input for our more advanced super- vised learning models.

Additionally, we leverage supervised learning to predict stock price movements of companies listed in the S&P 500, based on the content of their 10-K filings for the fiscal year 2022. The binary response variable we’re focused on indicates whether a specific company’s stock price either rises or falls during the month of its filing, which spans from December 1st, 2022, to December 31st, 2022. The 10K filings of S&P 500 companies are particularly relevant as they offer comprehensive financial information, management insights, and potential risk factors. Given the influence these factors can have on investor sentiment, analyzing the textual content of these filings can provide crucial insights that might not be evident from purely numerical data, making it instrumental in predicting stock market dynamics. The supervised learning models that we plan to implement to predict the forecast model are Random Forests and Logistic Regression.

### FINDINGS

The project explores ways to perform binary classification on S&P 500 companies’ stock price move- ments based on their 10-k filings. We first employ unsupervised learning methods including K- Means Clustering and LSA to examine the data to gain insights on the ideal number of partitioning and the major topics of our 10-k filings texts. Initially, we aimed to have two clusters (K = 2) in our K-Means analysis since our classification task is binary. However, our exploration revealed that the optimal number of clusters appears to be K = 5, as indicated by the word clouds. This suggests that our dataset holds more complexity and substructure than the simple binary classification problem we seek to address. The data could represent various subgroups, such as different industries or company sizes, which do not neatly fit into just two categories. Nevertheless, that doesn’t change our binary classification task.

The word clouds generated from these clusters mostly share financial terminologies, highlighting the limitation of K-Means Clustering in capturing the semantic nuances of textual data. In contrast, the LSA topics provide a more direct correlation with price movements. One topic comprises pos- itive words like ”increase,” directly implying an upward price movement, while three other topics feature negative terms like ”risk,” ”loss,” and ”catastrophe,” indicating a downward price trend. Interestingly, the LSA configuration that best aligns with our context also suggests K = 5. Although these topics are not binary and reflect a more complex structure, that still doesn’t change our binary classification task. In fact, we intend to leverage these unsupervised learning assignments for fea- ture engineering to reduce the dimensionality of our data and specifically for LSA to also capture the semantic meanings of our text. This reduction could potentially enhance the performance of our binary classification model when we transition to supervised learning.

The subsequent phase of our project focuses on the binary classification task, where we employ two supervised learning methods, namely Random Forest and Logistic Regression, across five different text representations: word frequency, sklearn TF-IDF, manual TF-IDF, gensim, and Latent Semantic Analysis (LSA). A notable distinction among these representations is that gensim and LSA have the capacity to capture semantic relationships within our text documents, which the others do not. As we observed in both the unsupervised learning and the feature importance part of the super- vised learning segments of the project, the partitioning of our text data without considering these semantic relationships may introduce inaccuracies in predicting our binary labels. Therefore, we anticipate that our supervised learning models will achieve higher accuracy when using gensim and LSA.
Our Logistic Regression results confirm this expectation, demonstrating that gensim and LSA sig- nificantly outperform the other representations. In contrast, the Random Forest results appear less clear. For one, the Random Forest results exhibit striking similarities across these representations. For another, amidst these similar scores, it assigns the highest importance to word frequency rather than gensim or LSA. This behavior can be attributed to the robustness and flexibility of the Random Forest algorithm, which can accommodate variations in data representation.

In conclusion, our study underscores the important need for our text representations to effectively capture semantic meanings to enable the successful performance of binary classification models, particularly when employing Random Forest and Logistic Regression. However, it’s important to acknowledge certain limitations in our findings.

Firstly, gensim and LSA, while promising in their ability to capture semantic relationships, do not generate features in the same manner as traditional methods. This absence of traditional feature importance assessment makes it challenging to verify the direct relevance of these features to price movements within the given context. Additionally, it’s important to acknowledge that our binary classification models, while effective, may not capture more nuanced or subtle relationships be- tween text representations and price movements. Furthermore, the choice of classifiers may impact the model’s performance, and alternative algorithms could yield different results. Moreover, the quality and quantity of the data used in training these models can significantly influence their per- formance and generalizability. A more extensive dataset or data preprocessing techniques could potentially address some of the limitations.

In future research, it might be beneficial to explore alternative feature engineering methods or ex- periment with different machine learning algorithms to improve the model’s ability to capture and interpret semantic information. Understanding these limitations is crucial for refining our approach and obtaining more reliable results in the context of predicting price movements.

